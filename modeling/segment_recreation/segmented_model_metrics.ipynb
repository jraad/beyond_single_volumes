{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b1f9512-76b7-4d34-9e80-8077fdcd6d87",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "350b618c-2da4-40c6-bdbb-7df94228c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.distributions as dist\n",
    "from torchsummary import summary\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import re\n",
    "from skimage.metrics  import structural_similarity as ssim\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "pio.renderers.default = 'iframe'\n",
    "\n",
    "from importlib import reload\n",
    "import visualization\n",
    "\n",
    "# locals\n",
    "import model_architectures\n",
    "\n",
    "reload(model_architectures)\n",
    "from model_architectures import VAESegment, Data3DSegToSeg, SegMaskData, Data3DSingleSegToSingleSeg, Data3DSegToSegT1, Data3DSingleSegToSingleSegT1\n",
    "\n",
    "reload(visualization)\n",
    "from visualization import brain_diff, viz_slices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09283324-0f7e-415e-9657-bffe1da6ca55",
   "metadata": {},
   "source": [
    "### Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37ccaa36-3018-402b-b534-05788f34c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_dir = r\"D:/school/research\"\n",
    "code_dir = os.path.join(research_dir, \"code\")\n",
    "p2_dir = os.path.join(code_dir, \"paper_two_code\")\n",
    "model_dir = os.path.join(p2_dir, \"models\")\n",
    "data_dir = os.path.join(research_dir, \"data\")\n",
    "dhcp_rel2 = os.path.join(data_dir, \"dhcp_rel2\")\n",
    "processed_dir = os.path.join(dhcp_rel2, \"processed\")\n",
    "volume_dir = os.path.join(processed_dir, \"volumes\")\n",
    "seg_dir = os.path.join(processed_dir, \"segments\")\n",
    "seg_vol_dir = os.path.join(processed_dir, \"volume_segments\")\n",
    "pred_dir = os.path.join(dhcp_rel2, \"predictions\")\n",
    "seg_pred_dir = os.path.join(pred_dir, \"vae_9seg\")\n",
    "metrics_dir = os.path.join(p2_dir, \"metrics\")\n",
    "seg_metrics_dir = os.path.join(metrics_dir, \"seg_to_seg\")\n",
    "\n",
    "l1_dir = os.path.join(volume_dir, \"l1\")\n",
    "l5_dir = os.path.join(volume_dir, \"l5\")\n",
    "\n",
    "l1_seg_dir = os.path.join(seg_dir, \"l1\")\n",
    "l5_seg_dir = os.path.join(seg_dir, \"l5\")\n",
    "\n",
    "l1_seg_vol_dir = os.path.join(seg_vol_dir, \"l1\")\n",
    "l5_seg_vol_dir = os.path.join(seg_vol_dir, \"l5\")\n",
    "\n",
    "l1_seg_pred_dir = os.path.join(seg_pred_dir, \"l1\")\n",
    "l5_seg_pred_dir = os.path.join(seg_pred_dir, \"l5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66d754-7e2d-472b-b0f4-59b2a2a8fbda",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d908be28-f074-4d44-8a84-9027eff95d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = [\n",
    "    \"Cerebrospinal Fluid\",\n",
    "    \"Cortical Grey Matter\",\n",
    "    \"White Matter\",\n",
    "    \"Background\",\n",
    "    \"Ventricle\",\n",
    "    \"Cerebelum\",\n",
    "    \"Deep Grey Matter\",\n",
    "    \"Brainstem\",\n",
    "    \"Hippocampus\"\n",
    "]\n",
    "\n",
    "std_bands = [\n",
    "    {\n",
    "        \"range\": \"-inf to -4\",\n",
    "        \"low\": -np.inf,\n",
    "        \"high\": -4\n",
    "    },\n",
    "    {\n",
    "        \"range\": \"-4 to -3\",\n",
    "        \"low\": -4,\n",
    "        \"high\": -3\n",
    "    },\n",
    "    {\n",
    "        \"range\": \"-3 to -2\",\n",
    "        \"low\": -3,\n",
    "        \"high\": -2\n",
    "    },\n",
    "    {\n",
    "        \"range\": \"2 to 3\",\n",
    "        \"low\": 2,\n",
    "        \"high\": 3\n",
    "    },\n",
    "    {\n",
    "        \"range\": \"3 to 4\",\n",
    "        \"low\": 3,\n",
    "        \"high\": 4\n",
    "    },\n",
    "    {\n",
    "        \"range\": \"4 to inf\",\n",
    "        \"low\": 4,\n",
    "        \"high\": np.inf\n",
    "    }\n",
    "]\n",
    "\n",
    "def connected_components_2d(diff):\n",
    "    _, labels = cv2.connectedComponents(diff)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "def calculate_clusters(diff, threshold=2):\n",
    "    cluster_diff = diff.copy()\n",
    "    filter_mask = (diff > -threshold) & (diff < threshold)\n",
    "    cluster_diff[filter_mask] = 0\n",
    "    cluster_diff[cluster_diff != 0] = 1\n",
    "    prepared_diff = cluster_diff.astype('uint8')\n",
    "    sizes = []\n",
    "    \n",
    "    for idx in range(0, 256):\n",
    "        clusters = connected_components_2d(prepared_diff[:,:,idx])\n",
    "        values, counts = np.unique(clusters, return_counts=True)\n",
    "        sizes.extend(counts[1:])\n",
    "        \n",
    "    return sizes\n",
    "\n",
    "\n",
    "def get_overall_metrics(model, data):\n",
    "    criterion = nn.MSELoss()\n",
    "    losses = []\n",
    "    segment_losses = {s: [] for s in segments}\n",
    "    clusters = []\n",
    "    segment_clusters = {s: [] for s in segments}\n",
    "    banded_metrics = {}\n",
    "    for band in std_bands:\n",
    "        banded_metrics[f\"{band['range']} pixels\"] = []\n",
    "        banded_metrics[f\"{band['range']} clusters\"] = []\n",
    "    \n",
    "    banded_segment_metrics = {}\n",
    "    for band in std_bands:\n",
    "        banded_segment_metrics[f\"{band['range']} pixels\"] = {s: [] for s in segments}\n",
    "        banded_segment_metrics[f\"{band['range']} clusters\"] = {s: [] for s in segments}\n",
    "        \n",
    "    counter = 0\n",
    "    for sample in data:\n",
    "        x = torch.Tensor(sample).reshape((1,) + sample.shape).cuda()\n",
    "        pred = model(x)\n",
    "        \n",
    "        losses.append(float(criterion(x, pred).cpu()))\n",
    "        \n",
    "        for idx, segment in enumerate(segments):\n",
    "            segment_losses[segment].append(float(criterion(x[:,idx,:,:,:], pred[:,idx,:,:,:]).cpu()))\n",
    "        \n",
    "        # Get overall cluster data\n",
    "        og = sample\n",
    "        pred = pred.reshape(og.shape).detach().cpu().numpy()\n",
    "        \n",
    "        diff = np.sum(og, axis=0) - np.sum(pred, axis=0)\n",
    "        diff_norm = diff / 0.1\n",
    "        \n",
    "        clusters.append(np.mean(calculate_clusters(diff_norm)))\n",
    "        \n",
    "        # Get segmented cluster data\n",
    "        for idx, segment in enumerate(segments):\n",
    "            diff = og[idx,:,:,:] - pred[idx,:,:,:]\n",
    "            diff_norm = diff / 0.1\n",
    "            segment_clusters[segment].append(np.mean(calculate_clusters(diff_norm)))\n",
    "        \n",
    "        # For overall banded metrics\n",
    "        for band in std_bands:\n",
    "            diff = np.sum(og, axis=0) - np.sum(pred, axis=0)\n",
    "            diff_norm = diff / 0.1\n",
    "            filter_mask = (diff_norm > band[\"low\"]) & (diff_norm < band[\"high\"])\n",
    "            banded_diff = diff_norm.copy()\n",
    "            banded_diff[np.invert(filter_mask)] = 0\n",
    "            banded_diff[banded_diff != 0] = 1\n",
    "            banded_diff = banded_diff.astype('uint8')\n",
    "            \n",
    "            banded_metrics[f\"{band['range']} pixels\"].append(np.sum(banded_diff))\n",
    "            \n",
    "            sizes = []\n",
    "            for idx in range(0, 256):\n",
    "                cc = connected_components_2d(banded_diff[:,:,idx])\n",
    "                values, counts = np.unique(cc, return_counts=True)\n",
    "                sizes.extend(counts[1:])\n",
    "            banded_metrics[f\"{band['range']} clusters\"].append(np.mean(sizes))\n",
    "        \n",
    "        # For segmented banded metrics\n",
    "        for band in std_bands:\n",
    "            for idx, segment in enumerate(segments):\n",
    "                diff = og[idx,:,:,:] - pred[idx,:,:,:]\n",
    "                diff_norm = diff / 0.1\n",
    "                \n",
    "                filter_mask = (diff_norm > band[\"low\"]) & (diff_norm < band[\"high\"])\n",
    "                banded_diff = diff_norm.copy()\n",
    "                banded_diff[np.invert(filter_mask)] = 0\n",
    "                banded_diff[banded_diff != 0] = 1\n",
    "                banded_diff = banded_diff.astype('uint8')\n",
    "                \n",
    "                banded_segment_metrics[f\"{band['range']} pixels\"][segment].append(np.sum(banded_diff))\n",
    "                \n",
    "                sizes = []\n",
    "                for idx in range(0, 256):\n",
    "                    cc = connected_components_2d(banded_diff[:,:,idx])\n",
    "                    values, counts = np.unique(cc, return_counts=True)\n",
    "                    sizes.extend(counts[1:])\n",
    "                if len(sizes) == 0:\n",
    "                    banded_segment_metrics[f\"{band['range']} clusters\"][segment].append(0)\n",
    "                else:\n",
    "                    banded_segment_metrics[f\"{band['range']} clusters\"][segment].append(np.mean(sizes))\n",
    "\n",
    "    return losses, segment_losses, clusters, segment_clusters, banded_metrics, banded_segment_metrics\n",
    "\n",
    "\n",
    "def save_values(losses, segment_losses, clusters, segment_clusters, banded_metrics, banded_segment_metrics, name, run):\n",
    "    prefix = f\"{name}_{run}\"\n",
    "    np.save(os.path.join(seg_metrics_dir, f\"{prefix}_losses.npy\"), losses)\n",
    "    np.save(os.path.join(seg_metrics_dir, f\"{prefix}_segment_losses.npy\"), segment_losses)\n",
    "    np.save(os.path.join(seg_metrics_dir, f\"{prefix}_clusters.npy\"), clusters)\n",
    "    np.save(os.path.join(seg_metrics_dir, f\"{prefix}_segment_clusters.npy\"), segment_clusters)\n",
    "    with open(os.path.join(seg_metrics_dir, f\"{prefix}_banded_metrics.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(banded_metrics,f)\n",
    "    with open(os.path.join(seg_metrics_dir, f\"{prefix}_banded_segment_metrics.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(banded_segment_metrics,f)\n",
    "\n",
    "\n",
    "def get_single_seg_metrics(model, data):\n",
    "    criterion = nn.MSELoss()\n",
    "    losses = []\n",
    "    clusters = []\n",
    "    banded_metrics = {}\n",
    "    for band in std_bands:\n",
    "        banded_metrics[f\"{band['range']} pixels\"] = []\n",
    "        banded_metrics[f\"{band['range']} clusters\"] = []\n",
    "\n",
    "    for sample in data:\n",
    "        x = torch.Tensor(sample).reshape((1,) + sample.shape).cuda()\n",
    "        pred = model(x)\n",
    "        \n",
    "        losses.append(float(criterion(x, pred).cpu()))\n",
    "        \n",
    "        og = sample\n",
    "        pred = pred.reshape(og.shape).detach().cpu().numpy()\n",
    "        \n",
    "        diff = np.sum(og, axis=0) - np.sum(pred, axis=0)\n",
    "        diff_norm = diff / 0.1\n",
    "        \n",
    "        clusters.append(np.mean(calculate_clusters(diff_norm)))\n",
    "        \n",
    "        for band in std_bands:\n",
    "            filter_mask = (diff_norm > band[\"low\"]) & (diff_norm < band[\"high\"])\n",
    "            banded_diff = diff_norm.copy()\n",
    "            banded_diff[np.invert(filter_mask)] = 0\n",
    "            banded_diff[banded_diff != 0] = 1\n",
    "            banded_diff = banded_diff.astype('uint8')\n",
    "            \n",
    "            banded_metrics[f\"{band['range']} pixels\"].append(np.sum(banded_diff))\n",
    "            \n",
    "            sizes = []\n",
    "            for idx in range(0, 256):\n",
    "                cc = connected_components_2d(banded_diff[:,:,idx])\n",
    "                values, counts = np.unique(cc, return_counts=True)\n",
    "                sizes.extend(counts[1:])\n",
    "            banded_metrics[f\"{band['range']} clusters\"].append(np.mean(sizes))\n",
    "\n",
    "    return losses, clusters, banded_metrics\n",
    "\n",
    "def save_values(losses, segment_losses, clusters, segment_clusters, banded_metrics, banded_segment_metrics, name, run):\n",
    "    prefix = f\"{name}_{run}\"\n",
    "    np.save(os.path.join(seg_metrics_dir, f\"{prefix}_losses.npy\"), losses)\n",
    "    np.save(os.path.join(seg_metrics_dir, f\"{prefix}_segment_losses.npy\"), segment_losses)\n",
    "    np.save(os.path.join(seg_metrics_dir, f\"{prefix}_clusters.npy\"), clusters)\n",
    "    np.save(os.path.join(seg_metrics_dir, f\"{prefix}_segment_clusters.npy\"), segment_clusters)\n",
    "    with open(os.path.join(seg_metrics_dir, f\"{prefix}_banded_metrics.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(banded_metrics,f)\n",
    "    with open(os.path.join(seg_metrics_dir, f\"{prefix}_banded_segment_metrics.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(banded_segment_metrics,f)\n",
    "\n",
    "        \n",
    "def save_seg_values(losses, clusters, banded_metrics, name, run):\n",
    "    prefix = f\"{name}_{run}\"\n",
    "    np.save(os.path.join(seg_metrics_dir, f\"{prefix}_losses.npy\"), losses)\n",
    "    np.save(os.path.join(seg_metrics_dir, f\"{prefix}_clusters.npy\"), clusters)\n",
    "    with open(os.path.join(seg_metrics_dir, f\"{prefix}_banded_metrics.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(banded_metrics,f)\n",
    "    \n",
    "def get_overall_metrics_single_seg(base_model_name, data):\n",
    "    criterion = nn.MSELoss()\n",
    "    losses = []\n",
    "    clusters = []\n",
    "    banded_metrics = {}\n",
    "    for band in std_bands:\n",
    "        banded_metrics[f\"{band['range']} pixels\"] = []\n",
    "        banded_metrics[f\"{band['range']} clusters\"] = []\n",
    "        \n",
    "    for sample in data:\n",
    "        pred_img = np.empty_like(sample)\n",
    "        for segment_number in range(0, len(segments)):\n",
    "            model_path = os.path.join(model_dir, f\"{base_model_name}{segment_number}.pt\")\n",
    "            model = VAESegment(1, 1)\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            model.cuda()\n",
    "            model.eval()\n",
    "\n",
    "            x = torch.Tensor(sample[segment_number]).reshape((1, 1, 256, 256, 256)).cuda()\n",
    "            pred = model(x)\n",
    "            pred_img[segment_number] = pred.reshape((256, 256, 256)).detach().cpu().numpy()\n",
    "\n",
    "        losses.append(float(criterion(torch.Tensor(sample), torch.Tensor(pred_img))))\n",
    "\n",
    "        diff = np.sum(sample, axis=0) - np.sum(pred_img, axis=0)\n",
    "        diff_norm = diff / 0.1\n",
    "\n",
    "        clusters.append(np.mean(calculate_clusters(diff_norm)))\n",
    "\n",
    "        # For overall banded metrics\n",
    "        for band in std_bands:\n",
    "            diff = np.sum(sample, axis=0) - np.sum(pred_img, axis=0)\n",
    "            diff_norm = diff / 0.1\n",
    "            filter_mask = (diff_norm > band[\"low\"]) & (diff_norm < band[\"high\"])\n",
    "            banded_diff = diff_norm.copy()\n",
    "            banded_diff[np.invert(filter_mask)] = 0\n",
    "            banded_diff[banded_diff != 0] = 1\n",
    "            banded_diff = banded_diff.astype('uint8')\n",
    "\n",
    "            banded_metrics[f\"{band['range']} pixels\"].append(np.sum(banded_diff))\n",
    "\n",
    "            sizes = []\n",
    "            for idx in range(0, 256):\n",
    "                cc = connected_components_2d(banded_diff[:,:,idx])\n",
    "                values, counts = np.unique(cc, return_counts=True)\n",
    "                sizes.extend(counts[1:])\n",
    "            banded_metrics[f\"{band['range']} clusters\"].append(np.mean(sizes))\n",
    "        \n",
    "    return losses, clusters, banded_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4167dadb-6377-4d48-b667-45d63a6a290e",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74ba02ad-49cd-44b7-a258-cf77b20c9544",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "num_samples = int(len(os.listdir(l1_dir)) / 2)\n",
    "samples = np.array([i for i in range(0, num_samples)])\n",
    "np.random.shuffle(samples)\n",
    "\n",
    "split_val = int(0.8 * num_samples)\n",
    "train_indices = samples[0:split_val]\n",
    "val_indices = samples[split_val:]\n",
    "\n",
    "num_test = int(len(os.listdir(l5_dir)) / 2)\n",
    "test_indices = np.array([i for i in range(0, num_test)])\n",
    "\n",
    "train = Data3DSegToSeg(l1_dir, l1_seg_vol_dir, train_indices)\n",
    "val = Data3DSegToSeg(l1_dir, l1_seg_vol_dir, val_indices)\n",
    "test = Data3DSegToSeg(l5_dir, l5_seg_vol_dir, test_indices)\n",
    "\n",
    "train_segments = SegMaskData(l1_seg_dir, train_indices)\n",
    "val_segments = SegMaskData(l1_seg_dir, val_indices)\n",
    "test_segments = SegMaskData(l5_seg_dir, test_indices)\n",
    "\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(train, batch_size=batch_size)#, num_workers=1)\n",
    "val_loader = DataLoader(val, batch_size=batch_size)#, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd6ddb-864c-4ad8-8b75-26681369a357",
   "metadata": {},
   "source": [
    "### Get T2 9 Seg to 9 Seg Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c37d93ca-c503-4e55-81de-b85cc011336a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting train metrics\n",
      "Collecting val metrics\n",
      "Collecting test metrics\n",
      "CPU times: total: 1d 12h 49min 35s\n",
      "Wall time: 1h 53min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_path = os.path.join(model_dir, \"vae_rel2t2_seg9_to_seg9.pt\")\n",
    "model = VAESegment(9, 9)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "print(\"Collecting train metrics\")\n",
    "train_losses, train_segment_losses, train_clusters, train_segment_clusters, train_banded_metrics, train_banded_segment_metrics = get_overall_metrics(model, train)\n",
    "save_values(train_losses, train_segment_losses, train_clusters, train_segment_clusters, train_banded_metrics, train_banded_segment_metrics, \"seg9_to_seg9\", \"train\")\n",
    "\n",
    "print(\"Collecting val metrics\")\n",
    "val_losses, val_segment_losses, val_clusters, val_segment_clusters, val_banded_metrics, val_banded_segment_metrics = get_overall_metrics(model, val)\n",
    "save_values(val_losses, val_segment_losses, val_clusters, val_segment_clusters, val_banded_metrics, val_banded_segment_metrics, \"seg9_to_seg9\", \"val\")\n",
    "\n",
    "print(\"Collecting test metrics\")\n",
    "test_losses, test_segment_losses, test_clusters, test_segment_clusters, test_banded_metrics, test_banded_segment_metrics = get_overall_metrics(model, test)\n",
    "save_values(test_losses, test_segment_losses, test_clusters, test_segment_clusters, test_banded_metrics, test_banded_segment_metrics, \"seg9_to_seg9\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916c3f01-d4bb-4c7a-9979-43315a36091f",
   "metadata": {},
   "source": [
    "### Get T1 9 Seg to 9 Seg Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6bfbb6d-b50b-4515-bcf1-77c14815628a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting train metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\compute\\envs\\torch\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning:\n",
      "\n",
      "Mean of empty slice.\n",
      "\n",
      "C:\\compute\\envs\\torch\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting val metrics\n",
      "Collecting test metrics\n",
      "CPU times: total: 1d 13h 21s\n",
      "Wall time: 1h 55min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_path = os.path.join(model_dir, \"vae_rel2t1_seg9_to_seg9.pt\")\n",
    "model = VAESegment(9, 9)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "train = Data3DSegToSegT1(l1_dir, l1_seg_vol_dir, train_indices)\n",
    "val = Data3DSegToSegT1(l1_dir, l1_seg_vol_dir, val_indices)\n",
    "test = Data3DSegToSegT1(l5_dir, l5_seg_vol_dir, test_indices)\n",
    "\n",
    "print(\"Collecting train metrics\")\n",
    "train_losses, train_segment_losses, train_clusters, train_segment_clusters, train_banded_metrics, train_banded_segment_metrics = get_overall_metrics(model, train)\n",
    "save_values(train_losses, train_segment_losses, train_clusters, train_segment_clusters, train_banded_metrics, train_banded_segment_metrics, \"seg9_to_seg9_t1\", \"train\")\n",
    "\n",
    "print(\"Collecting val metrics\")\n",
    "val_losses, val_segment_losses, val_clusters, val_segment_clusters, val_banded_metrics, val_banded_segment_metrics = get_overall_metrics(model, val)\n",
    "save_values(val_losses, val_segment_losses, val_clusters, val_segment_clusters, val_banded_metrics, val_banded_segment_metrics, \"seg9_to_seg9_t1\", \"val\")\n",
    "\n",
    "print(\"Collecting test metrics\")\n",
    "test_losses, test_segment_losses, test_clusters, test_segment_clusters, test_banded_metrics, test_banded_segment_metrics = get_overall_metrics(model, test)\n",
    "save_values(test_losses, test_segment_losses, test_clusters, test_segment_clusters, test_banded_metrics, test_banded_segment_metrics, \"seg9_to_seg9_t1\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a36ec-55cf-4fd1-9f3b-9490244971e6",
   "metadata": {},
   "source": [
    "### Get T2 1 Seg to 1 Seg Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fd12a770-37f9-4878-993d-0fd8ea778037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyze model for segment 0\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 1\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 2\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 3\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 4\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 5\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 6\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 7\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 8\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "CPU times: total: 1d 9h 44min 32s\n",
      "Wall time: 2h 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for segment_number in range(0, len(segments)):\n",
    "    print(f\"Analyze model for segment {segment_number}\")\n",
    "    \n",
    "    # Load data for segment\n",
    "    train = Data3DSingleSegToSingleSeg(l1_dir, l1_seg_vol_dir, train_indices, segment=segment_number)\n",
    "    val = Data3DSingleSegToSingleSeg(l1_dir, l1_seg_vol_dir, val_indices, segment=segment_number)\n",
    "    test = Data3DSingleSegToSingleSeg(l5_dir, l5_seg_vol_dir, test_indices, segment=segment_number)\n",
    "    \n",
    "    # Define output paths now :)\n",
    "    model_path = os.path.join(model_dir, f\"vae_rel2t2_seg_to_seg{segment_number}.pt\")\n",
    "    model = VAESegment(1, 1)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Train\")\n",
    "    train_losses, train_clusters, train_banded_metrics = get_single_seg_metrics(model, train)\n",
    "    save_seg_values(train_losses, train_clusters, train_banded_metrics, f\"seg{segment_number}\", \"train\")\n",
    "    \n",
    "    print(\"Validation\")\n",
    "    val_losses, val_clusters, val_banded_metrics = get_single_seg_metrics(model, val)\n",
    "    save_seg_values(val_losses, val_clusters, val_banded_metrics, f\"seg{segment_number}\", \"val\")\n",
    "    \n",
    "    print(\"Test\")\n",
    "    test_losses, test_clusters, test_banded_metrics = get_single_seg_metrics(model, test)\n",
    "    save_seg_values(test_losses, test_clusters, test_banded_metrics, f\"seg{segment_number}\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a8f85a-c0bc-4248-9ba6-1ced50fc0d0c",
   "metadata": {},
   "source": [
    "### Get T1 1 Seg to 1 Seg Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c6b7cd0-961e-4c94-bdf1-aba520632bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyze model for segment 0\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 1\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 2\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 3\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 4\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 5\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 6\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 7\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "Analyze model for segment 8\n",
      "Train\n",
      "Validation\n",
      "Test\n",
      "CPU times: total: 1d 9h 13min 29s\n",
      "Wall time: 1h 56min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for segment_number in range(0, len(segments)):\n",
    "    print(f\"Analyze model for segment {segment_number}\")\n",
    "    \n",
    "    # Load data for segment\n",
    "    train = Data3DSingleSegToSingleSegT1(l1_dir, l1_seg_vol_dir, train_indices, segment=segment_number)\n",
    "    val = Data3DSingleSegToSingleSegT1(l1_dir, l1_seg_vol_dir, val_indices, segment=segment_number)\n",
    "    test = Data3DSingleSegToSingleSegT1(l5_dir, l5_seg_vol_dir, test_indices, segment=segment_number)\n",
    "    \n",
    "    # Define output paths now :)\n",
    "    model_path = os.path.join(model_dir, f\"vae_rel2t1_seg_to_seg{segment_number}.pt\")\n",
    "    model = VAESegment(1, 1)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Train\")\n",
    "    train_losses, train_clusters, train_banded_metrics = get_single_seg_metrics(model, train)\n",
    "    save_seg_values(train_losses, train_clusters, train_banded_metrics, f\"seg{segment_number}_t1\", \"train\")\n",
    "    \n",
    "    print(\"Validation\")\n",
    "    val_losses, val_clusters, val_banded_metrics = get_single_seg_metrics(model, val)\n",
    "    save_seg_values(val_losses, val_clusters, val_banded_metrics, f\"seg{segment_number}_t1\", \"val\")\n",
    "    \n",
    "    print(\"Test\")\n",
    "    test_losses, test_clusters, test_banded_metrics = get_single_seg_metrics(model, test)\n",
    "    save_seg_values(test_losses, test_clusters, test_banded_metrics, f\"seg{segment_number}_t1\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca6a036-584d-408a-b416-2cf375297c1f",
   "metadata": {},
   "source": [
    "### Get T2 1 Seg to 1 Seg Overall Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbcda211-0939-49e6-9eb3-1d43a932c8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting train metrics\n",
      "Validation\n",
      "Test\n",
      "CPU times: total: 4h 22min 22s\n",
      "Wall time: 27min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train = Data3DSegToSeg(l1_dir, l1_seg_vol_dir, train_indices)\n",
    "val = Data3DSegToSeg(l1_dir, l1_seg_vol_dir, val_indices)\n",
    "test = Data3DSegToSeg(l5_dir, l5_seg_vol_dir, test_indices)\n",
    "\n",
    "print(\"Getting train metrics\")\n",
    "train_losses, train_clusters, train_banded_metrics = get_overall_metrics_single_seg(\"vae_rel2t2_seg_to_seg\", train)\n",
    "save_seg_values(train_losses, train_clusters, train_banded_metrics, \"overall_single_seg_t2\", \"train\")\n",
    "    \n",
    "print(\"Validation\")\n",
    "val_losses, val_clusters, val_banded_metrics = get_overall_metrics_single_seg(\"vae_rel2t2_seg_to_seg\", val)\n",
    "save_seg_values(val_losses, val_clusters, val_banded_metrics, \"overall_single_seg_t2\", \"val\")\n",
    "\n",
    "print(\"Test\")\n",
    "test_losses, test_clusters, test_banded_metrics = get_overall_metrics_single_seg(\"vae_rel2t2_seg_to_seg\", test)\n",
    "save_seg_values(test_losses, test_clusters, test_banded_metrics, \"overall_single_seg_t2\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac95d2c-cb8e-4953-9811-4181e508b467",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get T1 1 Seg to 1 Seg Overall Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22ad7ebe-1022-4d48-b9e4-051ea7f5837e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting train metrics\n",
      "Validation\n",
      "Test\n",
      "CPU times: total: 4h 22min 30s\n",
      "Wall time: 28min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train = Data3DSegToSegT1(l1_dir, l1_seg_vol_dir, train_indices)\n",
    "val = Data3DSegToSegT1(l1_dir, l1_seg_vol_dir, val_indices)\n",
    "test = Data3DSegToSegT1(l5_dir, l5_seg_vol_dir, test_indices)\n",
    "\n",
    "print(\"Getting train metrics\")\n",
    "train_losses, train_clusters, train_banded_metrics = get_overall_metrics_single_seg(\"vae_rel2t1_seg_to_seg\", train)\n",
    "save_seg_values(train_losses, train_clusters, train_banded_metrics, \"overall_single_seg_t1\", \"train\")\n",
    "    \n",
    "print(\"Validation\")\n",
    "val_losses, val_clusters, val_banded_metrics = get_overall_metrics_single_seg(\"vae_rel2t1_seg_to_seg\", val)\n",
    "save_seg_values(val_losses, val_clusters, val_banded_metrics, \"overall_single_seg_t1\", \"val\")\n",
    "\n",
    "print(\"Test\")\n",
    "test_losses, test_clusters, test_banded_metrics = get_overall_metrics_single_seg(\"vae_rel2t1_seg_to_seg\", test)\n",
    "save_seg_values(test_losses, test_clusters, test_banded_metrics, \"overall_single_seg_t1\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b59131-117b-427a-958e-713b6c62c472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
